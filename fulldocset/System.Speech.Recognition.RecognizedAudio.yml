### YamlMime:ManagedReference
items:
- uid: System.Speech.Recognition.RecognizedAudio
  id: RecognizedAudio
  children:
  - System.Speech.Recognition.RecognizedAudio.AudioPosition
  - System.Speech.Recognition.RecognizedAudio.Duration
  - System.Speech.Recognition.RecognizedAudio.Format
  - System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)
  - System.Speech.Recognition.RecognizedAudio.StartTime
  - System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)
  - System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)
  langs:
  - csharp
  name: RecognizedAudio
  nameWithType: RecognizedAudio
  fullName: System.Speech.Recognition.RecognizedAudio
  type: Class
  summary: "関連付けられている表しますオーディオの入力は、 <xref href=&quot;System.Speech.Recognition.RecognitionResult&quot;></xref>です。"
  remarks: "音声認識エンジンには、認識操作の一環としてオーディオ入力に関する情報が生成されます。 認識されているオーディオにアクセスするに使用して、<xref:System.Speech.Recognition.RecognitionResult.Audio%2A>プロパティまたは<xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A><xref:System.Speech.Recognition.RecognitionResult>。</xref:System.Speech.Recognition.RecognitionResult>メソッド</xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A></xref:System.Speech.Recognition.RecognitionResult.Audio%2A>       によって、次のイベントとメソッドの認識の結果を生成することができます、<xref:System.Speech.Recognition.SpeechRecognizer>と<xref:System.Speech.Recognition.SpeechRecognitionEngine>クラス:-イベント:-<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName>と<xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName>-<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName>と<xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName>-<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName>と<xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>-<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName>と<xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName>- <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName>-メソッド:-<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName>と<xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName>-<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName>と<xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName>- <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName>- <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName>> [!IMPORTANT] > A 認識エミュー レートされた音声認識によって生成される結果に認識されているオーディオが含まれていません</xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName></xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName></xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName></xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName></xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName></xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName></xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName></xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName></xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName></xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName></xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName></xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName></xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName></xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName></xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName></xref:System.Speech.Recognition.SpeechRecognitionEngine></xref:System.Speech.Recognition.SpeechRecognizer>。 このような認識結果では、その<xref:System.Speech.Recognition.RecognitionResult.Audio%2A>プロパティから返される`null`とその<xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A>メソッドが例外をスローします</xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A></xref:System.Speech.Recognition.RecognitionResult.Audio%2A>。 エミュレートされた音声認識機能の詳細については、次を参照してください、`EmulateRecognize`と`EmulateRecognizeAsync`のメソッド、<xref:System.Speech.Recognition.SpeechRecognizer>と<xref:System.Speech.Recognition.SpeechRecognitionEngine>クラス。</xref:System.Speech.Recognition.SpeechRecognitionEngine> </xref:System.Speech.Recognition.SpeechRecognizer> 。"
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>, or <xref:System.Speech.Recognition.Grammar.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public class RecognizedAudio
  inheritance:
  - System.Object
  implements: []
  inheritedMembers: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition
  id: AudioPosition
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: AudioPosition
  nameWithType: RecognizedAudio.AudioPosition
  fullName: System.Speech.Recognition.RecognizedAudio.AudioPosition
  type: Property
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "開始時に認識されているオーディオの入力オーディオ ストリーム内の場所を取得します。"
  remarks: "このプロパティは、認識された語句の入力デバイスの生成されたオーディオ ストリーム内の先頭の位置を参照します。 これに対し、`RecognizerAudioPosition`のプロパティ、<xref:System.Speech.Recognition.SpeechRecognitionEngine>と<xref:System.Speech.Recognition.SpeechRecognizer>クラスは、そのオーディオ入力内の認識エンジンの位置を参照します</xref:System.Speech.Recognition.SpeechRecognizer></xref:System.Speech.Recognition.SpeechRecognitionEngine>。 これらの位置は別にすることはできます。 詳細については、次を参照してください。[音声認識イベントを使用した](http://msdn.microsoft.com/en-us/01c598ca-2e0e-4e89-b303-cd1cef9e8482)です。       <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A>プロパティが認識操作の開始時に、システム時刻を取得します</xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A>。"
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public TimeSpan AudioPosition { get; }
    return:
      type: System.TimeSpan
      description: "認識されているオーディオの開始時に入力オーディオ ストリーム内の位置。"
  overload: System.Speech.Recognition.RecognizedAudio.AudioPosition*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.Duration
  id: Duration
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: Duration
  nameWithType: RecognizedAudio.Duration
  fullName: System.Speech.Recognition.RecognizedAudio.Duration
  type: Property
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "認識されたオーディオ入力オーディオ ストリームの期間を取得します。"
  remarks: ''
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public TimeSpan Duration { get; }
    return:
      type: System.TimeSpan
      description: "入力オーディオ ストリーム内で認識されているオーディオの期間です。"
  overload: System.Speech.Recognition.RecognizedAudio.Duration*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.Format
  id: Format
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: Format
  nameWithType: RecognizedAudio.Format
  fullName: System.Speech.Recognition.RecognizedAudio.Format
  type: Property
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "認識エンジンによって処理されたオーディオの形式を取得します。"
  remarks: ''
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public System.Speech.AudioFormat.SpeechAudioFormatInfo Format { get; }
    return:
      type: System.Speech.AudioFormat.SpeechAudioFormatInfo
      description: "音声認識エンジンによって処理されるオーディオの形式です。"
  overload: System.Speech.Recognition.RecognizedAudio.Format*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)
  id: GetRange(System.TimeSpan,System.TimeSpan)
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: GetRange(TimeSpan,TimeSpan)
  nameWithType: RecognizedAudio.GetRange(TimeSpan,TimeSpan)
  fullName: System.Speech.Recognition.RecognizedAudio.GetRange(TimeSpan,TimeSpan)
  type: Method
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "選択して、認識される現在のセクションのバイナリ データとしてオーディオを返します。"
  remarks: ''
  example:
  - "The following example creates a speech recognition grammar for name input, adds a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event, and loads the grammar into an in-process speech recognizer. Then it writes the audio information for the name portion of the input to an audio file. The audio file is used as input to a <xref:System.Speech.Synthesis.SpeechSynthesizer> object, which speaks a phrase that includes the recorded audio.  \n  \n```  \nprivate static void AddNameGrammar(SpeechRecognitionEngine recognizer)  \n{  \n  GrammarBuilder builder = new GrammarBuilder();  \n  builder.Append(\"My name is\");  \n  builder.AppendWildcard();  \n  \n  Grammar nameGrammar = new Grammar(builder);  \n  nameGrammar.Name = \"Name Grammar\";  \n  nameGrammar.SpeechRecognized +=  \n    new EventHandler<SpeechRecognizedEventArgs>(  \n      NameSpeechRecognized);  \n  \n  recognizer.LoadGrammar(nameGrammar);  \n}  \n  \n// Handle the SpeechRecognized event of the name grammar.  \nprivate static void NameSpeechRecognized(  \n  object sender, SpeechRecognizedEventArgs e)  \n{  \n  Console.WriteLine(\"Grammar ({0}) recognized speech: {1}\",  \n    e.Result.Grammar.Name, e.Result.Text);  \n  \n  try  \n  {  \n  \n    // The name phrase starts after the first three words.  \n    if (e.Result.Words.Count < 4)  \n    {  \n  \n      // Add code to check for an alternate that contains the wildcard.  \n      return;  \n    }  \n  \n    RecognizedAudio audio = e.Result.Audio;  \n    TimeSpan start = e.Result.Words[3].AudioPosition;  \n    TimeSpan duration = audio.Duration - start;  \n  \n    // Add code to verify and persist the audio.  \n    string path = @\"C:\\temp\\nameAudio.wav\";  \n    using (Stream outputStream = new FileStream(path, FileMode.Create))  \n    {  \n      RecognizedAudio nameAudio = audio.GetRange(start, duration);  \n      nameAudio.WriteToWaveStream(outputStream);  \n      outputStream.Close();  \n    }  \n  \n    Thread testThread =  \n      new Thread(new ParameterizedThreadStart(TestAudio));  \n    testThread.Start(path);  \n  }  \n  catch (Exception ex)  \n  {  \n    Console.WriteLine(\"Exception thrown while processing audio:\");  \n    Console.WriteLine(ex.ToString());  \n  }  \n}  \n  \n// Use the speech synthesizer to play back the .wav file  \n// that was created in the SpeechRecognized event handler.  \n  \nprivate static void TestAudio(object item)  \n{  \n  string path = item as string;  \n  if (path != null && File.Exists(path))  \n  {  \n    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  \n    PromptBuilder builder = new PromptBuilder();  \n    builder.AppendText(\"Hello\");  \n    builder.AppendAudio(path);  \n    synthesizer.Speak(builder);  \n  }  \n}  \n```"
  syntax:
    content: public System.Speech.Recognition.RecognizedAudio GetRange (TimeSpan audioPosition, TimeSpan duration);
    parameters:
    - id: audioPosition
      type: System.TimeSpan
      description: "返されるオーディオ データの開始点です。"
    - id: duration
      type: System.TimeSpan
      description: "返されるセグメントの長さ。"
    return:
      type: System.Speech.Recognition.RecognizedAudio
      description: "定義された、認識されているオーディオのサブセクションを返します<code> audioPosition </code>と<code> duration</code>です。"
  overload: System.Speech.Recognition.RecognizedAudio.GetRange*
  exceptions:
  - type: System.ArgumentOutOfRangeException
    commentId: T:System.ArgumentOutOfRangeException
    description: "<code>audioPosition</code>および<code>duration</code>オーディオが現在のセグメントの範囲外のセグメントを定義します。"
  - type: System.InvalidOperationException
    commentId: T:System.InvalidOperationException
    description: "現在は認識オーディオ データは含まれません。"
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.StartTime
  id: StartTime
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: StartTime
  nameWithType: RecognizedAudio.StartTime
  fullName: System.Speech.Recognition.RecognizedAudio.StartTime
  type: Property
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "認識操作の開始時に、システム時刻を取得します。"
  remarks: "StartTime プロパティは、待機時間とパフォーマンスの計算のために役立つ認識操作の開始時のシステム時刻を取得します。       <xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A>プロパティは、入力デバイスの生成されたオーディオ ストリーム内の場所を取得します</xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A>。"
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public DateTime StartTime { get; }
    return:
      type: System.DateTime
      description: "認識操作の開始時のシステム時刻。"
  overload: System.Speech.Recognition.RecognizedAudio.StartTime*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)
  id: WriteToAudioStream(System.IO.Stream)
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: WriteToAudioStream(Stream)
  nameWithType: RecognizedAudio.WriteToAudioStream(Stream)
  fullName: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(Stream)
  type: Method
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "生データとして、全体のオーディオをストリームに書き込みます。"
  remarks: "オーディオ データを書き込む`outputStream`バイナリ形式にします。 ヘッダー情報が含まれていない場合です。       WriteToAudioStream メソッドは、Wave 形式を使用しますが、Wave ヘッダーは含まれません。 Wave ヘッダーを含めるを使用して、<xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A>メソッド</xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A>。"
  syntax:
    content: public void WriteToAudioStream (System.IO.Stream outputStream);
    parameters:
    - id: outputStream
      type: System.IO.Stream
      description: "オーディオのデータを受信するストリーム。"
  overload: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)
  id: WriteToWaveStream(System.IO.Stream)
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: WriteToWaveStream(Stream)
  nameWithType: RecognizedAudio.WriteToWaveStream(Stream)
  fullName: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(Stream)
  type: Method
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "オーディオを Wave 形式のストリームに書き込みます。"
  remarks: "オーディオ データを書き込む`outputStream`を Wave 形式でリソース インターチェンジ ファイルの式 (RIFF) ヘッダーが含まれています。       <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A>メソッドは、同じバイナリ形式を使用しますが、Wave ヘッダーは含まれません</xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A>。"
  example:
  - "The following example creates a speech recognition grammar for name input, adds a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event, and loads the grammar into an in-process speech recognizer. Then it writes the audio information for the name portion of the input to an audio file. The audio file is used as input to a <xref:System.Speech.Synthesis.SpeechSynthesizer> object, which speaks a phrase that includes the recorded audio.  \n  \n```  \nprivate static void AddNameGrammar(SpeechRecognitionEngine recognizer)  \n{  \n  GrammarBuilder builder = new GrammarBuilder();  \n  builder.Append(\"My name is\");  \n  builder.AppendWildcard();  \n  \n  Grammar nameGrammar = new Grammar(builder);  \n  nameGrammar.Name = \"Name Grammar\";  \n  nameGrammar.SpeechRecognized +=  \n    new EventHandler<SpeechRecognizedEventArgs>(  \n      NameSpeechRecognized);  \n  \n  recognizer.LoadGrammar(nameGrammar);  \n}  \n  \n// Handle the SpeechRecognized event of the name grammar.  \nprivate static void NameSpeechRecognized(  \n  object sender, SpeechRecognizedEventArgs e)  \n{  \n  Console.WriteLine(\"Grammar ({0}) recognized speech: {1}\",  \n    e.Result.Grammar.Name, e.Result.Text);  \n  \n  try  \n  {  \n    // The name phrase starts after the first three words.  \n    if (e.Result.Words.Count < 4)  \n    {  \n  \n      // Add code to check for an alternate that contains the   \nwildcard.  \n      return;  \n    }  \n  \n    RecognizedAudio audio = e.Result.Audio;  \n    TimeSpan start = e.Result.Words[3].AudioPosition;  \n    TimeSpan duration = audio.Duration - start;  \n  \n    // Add code to verify and persist the audio.  \n    string path = @\"C:\\temp\\nameAudio.wav\";  \n    using (Stream outputStream = new FileStream(path, FileMode.Create))  \n    {  \n      RecognizedAudio nameAudio = audio.GetRange(start, duration);  \n      nameAudio.WriteToWaveStream(outputStream);  \n      outputStream.Close();  \n    }  \n  \n    Thread testThread =  \n      new Thread(new ParameterizedThreadStart(TestAudio));  \n    testThread.Start(path);  \n  }  \n  catch (Exception ex)  \n  {  \n    Console.WriteLine(\"Exception thrown while processing audio:\");  \n    Console.WriteLine(ex.ToString());  \n  }  \n}  \n  \n// Use the speech synthesizer to play back the .wav file  \n// that was created in the SpeechRecognized event handler.  \n  \nprivate static void TestAudio(object item)  \n{  \n  string path = item as string;  \n  if (path != null && File.Exists(path))  \n  {  \n    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  \n    PromptBuilder builder = new PromptBuilder();  \n    builder.AppendText(\"Hello\");  \n    builder.AppendAudio(path);  \n    synthesizer.Speak(builder);  \n  }  \n}  \n```"
  syntax:
    content: public void WriteToWaveStream (System.IO.Stream outputStream);
    parameters:
    - id: outputStream
      type: System.IO.Stream
      description: "オーディオのデータを受信するストリーム。"
  overload: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream*
  exceptions: []
  platform:
  - net462
references:
- uid: System.Object
  isExternal: false
  name: System.Object
- uid: System.ArgumentOutOfRangeException
  isExternal: true
  name: System.ArgumentOutOfRangeException
- uid: System.InvalidOperationException
  isExternal: true
  name: System.InvalidOperationException
- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: AudioPosition
  nameWithType: RecognizedAudio.AudioPosition
  fullName: System.Speech.Recognition.RecognizedAudio.AudioPosition
- uid: System.TimeSpan
  parent: System
  isExternal: true
  name: TimeSpan
  nameWithType: TimeSpan
  fullName: System.TimeSpan
- uid: System.Speech.Recognition.RecognizedAudio.Duration
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: Duration
  nameWithType: RecognizedAudio.Duration
  fullName: System.Speech.Recognition.RecognizedAudio.Duration
- uid: System.Speech.Recognition.RecognizedAudio.Format
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: Format
  nameWithType: RecognizedAudio.Format
  fullName: System.Speech.Recognition.RecognizedAudio.Format
- uid: System.Speech.AudioFormat.SpeechAudioFormatInfo
  parent: System.Speech.AudioFormat
  isExternal: false
  name: SpeechAudioFormatInfo
  nameWithType: SpeechAudioFormatInfo
  fullName: System.Speech.AudioFormat.SpeechAudioFormatInfo
- uid: System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: GetRange(TimeSpan,TimeSpan)
  nameWithType: RecognizedAudio.GetRange(TimeSpan,TimeSpan)
  fullName: System.Speech.Recognition.RecognizedAudio.GetRange(TimeSpan,TimeSpan)
- uid: System.Speech.Recognition.RecognizedAudio
  parent: System.Speech.Recognition
  isExternal: false
  name: RecognizedAudio
  nameWithType: RecognizedAudio
  fullName: System.Speech.Recognition.RecognizedAudio
- uid: System.Speech.Recognition.RecognizedAudio.StartTime
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: StartTime
  nameWithType: RecognizedAudio.StartTime
  fullName: System.Speech.Recognition.RecognizedAudio.StartTime
- uid: System.DateTime
  parent: System
  isExternal: true
  name: DateTime
  nameWithType: DateTime
  fullName: System.DateTime
- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: WriteToAudioStream(Stream)
  nameWithType: RecognizedAudio.WriteToAudioStream(Stream)
  fullName: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(Stream)
- uid: System.IO.Stream
  parent: System.IO
  isExternal: true
  name: Stream
  nameWithType: Stream
  fullName: System.IO.Stream
- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: WriteToWaveStream(Stream)
  nameWithType: RecognizedAudio.WriteToWaveStream(Stream)
  fullName: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(Stream)
- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: AudioPosition
  nameWithType: RecognizedAudio.AudioPosition
- uid: System.Speech.Recognition.RecognizedAudio.Duration*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: Duration
  nameWithType: RecognizedAudio.Duration
- uid: System.Speech.Recognition.RecognizedAudio.Format*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: Format
  nameWithType: RecognizedAudio.Format
- uid: System.Speech.Recognition.RecognizedAudio.GetRange*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: GetRange
  nameWithType: RecognizedAudio.GetRange
- uid: System.Speech.Recognition.RecognizedAudio.StartTime*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: StartTime
  nameWithType: RecognizedAudio.StartTime
- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: WriteToAudioStream
  nameWithType: RecognizedAudio.WriteToAudioStream
- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: WriteToWaveStream
  nameWithType: RecognizedAudio.WriteToWaveStream
